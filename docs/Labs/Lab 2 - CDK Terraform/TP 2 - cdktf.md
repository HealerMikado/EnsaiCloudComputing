---
layout: default
title: "TP 2 - CDK Terraform ü™ê"
nav_order: 2
parent: Labs
---

# TP 2 - Terraform ü™ê et la cr√©ation d'infrastructure avec du code üë©‚Äçüíª

## Mise en place

Allez sur la plateforme AWS Academy et acc√©dez au cours AWS Academy Learner Lab. Puis cliquez sur `Modules` > `Learner Lab`. Lancez votre environnement en cliquant sur `Start Lab`. Une fois le cercle pass√© au vert, cliquez sur `AWS Details` et `AWS CLI`. Les cl√©s que vous voyez vont permettre un acc√®s programmatique √† votre compte.

Ouvrez un terminal et ex√©cutez la commande `aws configure`. Un prompt va vous demander votre AWS Access Key ID, collez la valeur de `aws_access_key_id`. Faites de m√™me pour la Secret Access Key. Pour la r√©gion par d√©faut, entrez "us-east-1". Validez le dernier prompt. Allez ensuite modifier le fichier `credentials` qui se trouve dans le dossier `.aws` (attention ce dossier est cach√©).

Cr√©ez un dossier `cloud computing` avec la commande `mkdir "cloud computing"`. D√©placez-vous dans le dossier avec la commande `cd "cloud computing"`. Clonez le d√©p√¥t git du TP avec un `git clone https://github.com/HealerMikado/Ensai-CloudComputingLab2.git`. Au d√©but de chaque exercice, vous allez devoir r√©aliser un `pipenv sync` dans le dossier de l'exercice. Cela va cr√©er un environnement Python et y installer toutes les d√©pendances de l'exercice. Pour que VScode reconnaisse les modules que vous allez utiliser, il faut lui sp√©cifier l'interpr√©teur qu'il doit utiliser. Faites un `ctrl+shift+P`, tapez `Select Interpreter` et prenez l'interpr√©teur `pipenv ex X cdktf ...`.

> üì¶ √Ä cause du fonctionnement de Python, cela va multiplier les environnements virtuels et le stockage qui leur est associ√©. Une solution moins gourmande en espace disque mais plus "moche" est de r√©utiliser toujours le m√™me espace.

## Mon premier script avec le CDK Terraform

### Une instance de base

Ouvrez le fichier `main.py`. Il contient l'architecture minimale du code n√©cessaire pour que vous puissiez r√©aliser le TP.
```python
from constructs import Construct
from cdktf import App, TerraformStack, TerraformOutput
from cdktf_cdktf_provider_aws.provider import AwsProvider
from cdktf_cdktf_provider_aws.instance import Instance, InstanceEbsBlockDevice
from cdktf_cdktf_provider_aws.security_group import SecurityGroup, SecurityGroupIngress, SecurityGroupEgress
from user_data import user_data

class MyStack(TerraformStack):
    def __init__(self, scope: Construct, id: str):
        super().__init__(scope, id)

        AwsProvider(self, "AWS", region="us-east-1")
        TerraformOutput(
            self, "public_ip",
            value=instance.public_ip,
            )


app = App()
MyStack(app, "cloud_commputing")

app.synth()

```
Les imports sont tous les imports dont vous aurez besoin. Ils ne sont pas √©vidents √† trouver, donc je vous les donne.

La classe `MyStack` va contenir toute votre architecture. Pour associer les services que vous allez cr√©er √† votre *stack*, vous allez passer en param√®tre la stack courante √† tous vos objets. Ainsi, **tous les objets AWS que vous allez cr√©er vont avoir en premier argument `self`**.

Maintenant, vous allez d√©finir votre premi√®re ressource. La classe du cdktf associ√©e √† une instance EC2 d'AWS est la classe `Instance`. Les deux premiers arguments √† passer au constructeur de la classe `Instance` sont la stack courante et un id sous la forme d'une cha√Æne de caract√®res.

> üßô‚Äç‚ôÇÔ∏è Sauf rare exception, tous les objets que vous allez cr√©er vont avoir comme premiers arguments la stack courante et un id.

```python
instance = Instance(
    self, "webservice")
```

Ensuite, via des param√®tres nomm√©s, vous allez d√©finir un peu plus en d√©tail votre instance. Rappelez-vous, pour une instance EC2, il vous faut d√©finir son OS (appel√© AMI chez AWS) et le type d'instance.

Ajoutez √† votre instance son AMI avec le param√®tre `ami` qui prendra comme valeur `ami-0557a15b87f6559cf` (c'est l'identifiant de l'AMI Ubuntu dans la r√©gion `us-east-1`), et pour le type d'instance, vous prendrez une `t2.micro`. Ex√©cutez votre architecture avec la commande `cdktf deploy` dans le terminal. Connectez-vous au tableau de bord EC2 et v√©rifiez que votre instance est bien d√©marr√©e. N√©anmoins, si vous essayez de vous connecter en SSH √† votre instance, vous allez voir que c'est impossible. En effet, lors de la d√©finition de l'instance, nous n'avons pas d√©fini la cl√© SSH √† utiliser et le *security group* de l'instance. Tout cela fait que, pour le moment, l'instance n'est pas accessible.

### Configuration de la partie r√©seau

Vu que ce n'est pas int√©ressant √† trouver seul, voici le code pour d√©finir le *security group* de l'instance :

```python
security_group = SecurityGroup(
    self, "sg-tp",
    ingress=[
        SecurityGroupIngress(
            from_port=22,
            to_port=22,
            cidr_blocks=["0.0.0.0/0"],
            protocol="TCP",
            description="Accept incoming SSH connection"
        ),
        SecurityGroupIngress(
            from_port=80,
            to_port=80,
            cidr_blocks=["0.0.0.0/0"],
            protocol="TCP",
            description="Accept incoming HTTP connection"
        )
    ],
    egress=[
        SecurityGroupEgress(
            from_port=0,
            to_port=0,
            cidr_blocks=["0.0.0.0/0"],
            protocol="-1",
            description="allow all egresse connection"
        )
    ]
)
```
Ce *security group* n'accepte que les connexions HTTP et SSH en entr√©e et permet tout le trafic en sortie. Pour associer ce *security group* √† votre instance, vous allez devoir ajouter un param√®tre `security_groups` lors de la cr√©ation de l'objet. Attention, ce param√®tre attend une liste de *security groups*. Pour d√©finir la cl√©, ajoutez le param√®tre `key_name` avec comme valeur le nom de la cl√© (`vockey`). Vous pouvez maintenant relancer votre instance avec un nouveau `cdktf deploy`. Cela va r√©silier l'instance pr√©c√©dente et en cr√©er une nouvelle.
### Configuration des user data

Pour le moment, nous n'avons pas d√©fini les *user data* de l'instance. Pour les ajouter, il faut simplement ajouter le param√®tre `user_data_base64` avec comme valeur la variable contenue dans `user_data.py` (la valeur est d√©j√† import√©e). Relancez votre *stack* et, apr√®s quelques instants, vous pourrez vous connecter au webservice de l'instance. Utilisez l'IP qui s'affiche dans votre terminal apr√®s un `cdktf deploy`.

### Configuration des disques (bonus)

Actuellement, l'instance cr√©√©e n'a qu'un disque de 8 Go. C'est suffisant, mais il est possible de changer cela via Terraform. Par exemple, ajoutez ce bout de code √† votre instance.

```python
ebs_block_device= [InstanceEbsBlockDevice(
    device_name="/dev/sda1",
    delete_on_termination=True,
    encrypted=False,
    volume_size=20,
    volume_type="gp2"
),
InstanceEbsBlockDevice(
    device_name="/dev/sdb",
    delete_on_termination=True,
    encrypted=False,
    volume_size=100,
    volume_type="gp2"
)]
```
Le premier disque de l'instance aura ainsi un volume de 20 Go, et un second disque sera attach√© avec un volume de 100 Go. Les deux disques seront supprim√©s en m√™me temps que l'instance. Vous pouvez voir les deux disques en vous connectant √† l'instance en SSH et en ex√©cutant la commande `df` (*disk free*).

## Mise en place d'un Auto Scaling Group et d'un Load Balancer

Ci-dessous, vous trouverez l'architecture finale que vous allez mettre en place pour ce TP. Elle est un peu plus d√©taill√©e que lors du pr√©c√©dent TP pour faire appara√Ætre chaque √©l√©ment que vous allez devoir d√©finir. Se d√©tacher de l'interface graphique pour utiliser un outil IaC fait r√©aliser √† quel point la console AWS masque de nombreux d√©tails. Tout impl√©menter n'est pas difficile, mais est laborieux quand on n'est pas guid√©. Toutes les √©tapes sont d√©coup√©es pour √™tre unitaires et simples. Elles consistent toutes √† d√©finir un objet Python avec la bonne classe et les bons param√®tres. Ce n'est pas simple de trouver cela seul, alors je vous donne tout. Il suffit de suivre le TP √† votre rythme.

<img src="img/Architecuture finale.jpg" style="zoom: 50%;" />



Vous trouverez le code de cet exercice dans le dossier `ex 2 cdktf haute dispo`.

### Launch Template

La premi√®re √©tape va √™tre de d√©finir le *template* des instances de l'*Auto Scaling Group*. Pour cela, vous allez utiliser la classe `LaunchTemplate`. Comme un *template* est quasiment la m√™me chose qu'une instance, l'objet `LaunchTemplate` va fortement ressembler √† une instance, seuls les noms des param√®tres vont changer (oui, il n'y a pas de coh√©rence sur les noms). Ainsi, votre objet `LaunchTemplate` va avoir comme param√®tres :

- la stack courante,
- un id sous la forme d'un string,
- `image_id` qui va d√©finir son image AMI,
- `instance_type` qui va d√©finir le type d'instance,
- `user_data` qui va d√©finir les user data. Attention, m√™me si ce n'est pas pr√©cis√©, elles doivent bien √™tre encod√©es en base 64,
- `vpc_security_group_ids` au lieu de `security_groups` pour la liste des *security groups*,
- `key_name` pour la cl√© SSH √† utiliser.

### Auto Scaling Group

Maintenant que le *template* est d√©fini, c'est le moment de l'utiliser avec un *Auto Scaling Group*. Souvenez-vous, un *Auto Scaling Group* va maintenir un nombre d'instances compris entre le min et le max d√©fini. La classe qui repr√©sente un ASG est simplement `AutoscalingGroup`. Elle prend en param√®tres :

- la stack courante,
- un id sous la forme d'un string,
- `min_size`, `max_size` et `desired_capacity` pour la limite inf√©rieure, sup√©rieure, et la valeur initiale,
- `launch_template` qui permet de sp√©cifier le *template* √† utiliser. Vous pouvez passer un dictionnaire contenant uniquement la cl√© `id` avec comme valeur l'id du *launch template* que vous obtiendrez avec l'attribut `id` du *launch template*,
- `vpc_zone_identifier` pour sp√©cifier les sous-r√©seaux √† utiliser pour l'Auto Scaling Group. Utilisez la variable `subnets` pr√©sente dans le fichier.

Il ne vous reste plus qu'√† lancer votre code. Il va cr√©er les sous-r√©seaux n√©cessaires, un Launch Template et un ASG selon vos sp√©cifications. Attendez quelques instants puis allez sur le tableau de bord EC2, vous devriez voir appara√Ætre 3 instances.

### Elastic Load Balancer

Derni√®re pi√®ce √† d√©finir, le *Load Balancer* va avoir la charge de r√©partir les requ√™tes entre nos instances. La cr√©ation via l'interface a cach√© pas mal de choses et, au lieu de cr√©er un simple objet, il faut en cr√©er 3 :

- le *Load Balancer* en tant que tel,
- le *Target Group* qui va permettre de consid√©rer l'ASG comme une cible possible pour le *Load Balancer*,
- et un *Load Balancer Listener* pour relier les deux.

#### Load Balancer

D√©finir le *Load Balancer* est assez simple. Cela se fait avec la classe `Lb`. En plus des classiques stack courante et id, elle prend en param√®tre :

- son type avec le param√®tre `load_balancer_type`. Dans le cas pr√©sent, cela sera "application",
- les sous-r√©seaux avec lesquels il communique avec le param√®tre `subnets`. Prenez la valeur `subnets` d√©j√† d√©finie,
- et les groupes de s√©curit√© qui lui sont associ√©s avec le param√®tre `security_groups`. Le *security group* d√©j√† d√©fini convient tr√®s bien. Attention, ce param√®tre attend une liste.

#### Target Group

Le *Target Group* est √©galement facile. Utilisez la classe `LbTargetGroup` et passez la stack et un id. Il vous faut ensuite d√©finir les param√®tres :

- `port` en sp√©cifiant le port 80 et `protocol` en sp√©cifiant `HTTP` car nous voulons que le TG soit accessible uniquement en HTTP sur le port 80,
- `vpc_id` avec l'id du VPC d√©j√† d√©fini. Cela est n√©cessaire car cela permet √† AWS de savoir que les machines du *Target Group* seront dans le r√©seau.

Il faut maintenant associer votre *Target Group* √† votre ASG. Cela passe par l'ajout d'un attribut `target_group_arns` dans l'ASG. Cet attribut attend la liste des arn (Amazon Resource Names) des Target Groups. Votre *Target Group* expose son arn via l'attribut `arn`.

#### Load Balancer Listener

Il ne nous reste plus qu'√† dire au *Load Balancer* de forwarder les requ√™tes HTTP vers notre *Target Group*. Il faut utiliser l'objet `LbListener` pour √ßa. Il prend, en plus des param√®tres habituels :

- `load_balancer_arn` qui est l'arn du Load Balancer. Pour r√©cup√©rer l'arn de votre Load Balancer, utilisez l'attribut `arn`,
- `port` qui va prendre la valeur 80 car nous allons forwarder les requ√™tes faites sur le port 80,
- `protocol` qui va prendre la valeur HTTP car nous allons forwarder les requ√™tes HTTP,
- `default_action` o√π nous allons dire ce que nous voulons faire, ici forwarder les requ√™tes vers notre *Target Group*. Comme un *Load Balancer Listener* peut avoir plusieurs actions, ce param√®tre attend une liste. Ensuite, notre action de forward va se d√©finir via un autre objet dont voici le code `LbListenerDefaultAction(type="forward", target_group_arn=target_group.arn)`.

Vous pouvez maintenant relancer votre code avec un `cdktf deploy`, allez sur la page du load balancer, obtenir son adresse dns et acc√©der au endpoint `/instance`. Rafra√Æchissez la page et l'id affichez devrait changer r√©guli√®rement.

## Conclusion

Vous venez lors de ce TP de cr√©er via du code toute une infrastructure informatique. M√™me si cela n'est pas simple √† faire, le code que vous avez √©crit peut √™tre maintenant r√©utiliser √† l'infini et versionn√© via git. Il est ainsi partageable, et vous pouvez voir son √©volution. Il peut √©galement √™tre utilis√© dans un pipeline de CI/CD pour que l'architecture soit d√©ploy√©e automatiquement.

M√™me si les solution IaC ont des avantages, je ne vous les recommandes pas pour d√©couvrir un service. Explorer l'interface dans un premier temps pour voir les options disponibles permet de mieux comprendre le service. Automatiser la cr√©ation de service via du code par la suite si c'est n√©cessaire.